{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392f443-602b-49ee-8746-3150266685a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = \"Input.xlsx\"  # Replace with the path to your Excel file\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Sort the DataFrame by URL column\n",
    "df_sorted = df.sort_values(by='URL', ascending=True)\n",
    "\n",
    "# Print each row of the sorted URL column\n",
    "for url in df_sorted['URL']:\n",
    "    headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}\n",
    "    webpage = requests.get(url, headers=headers).text\n",
    "\n",
    "    soup = BeautifulSoup(webpage, 'lxml')\n",
    "    soup.find_all('h1')[0].text.strip('\\n''\\t')\n",
    "\n",
    "    div_tag = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "\n",
    "    # Check if the <div> tag exists\n",
    "    if div_tag:\n",
    "        # Remove the text within <pre> tags\n",
    "        for pre_tag in div_tag.find_all('pre'):\n",
    "            pre_tag.extract()\n",
    "\n",
    "        # Extract all the text from the <div> tag\n",
    "        text = div_tag.get_text(separator='\\n', strip=True)\n",
    "\n",
    "        # Print or use the extracted text\n",
    "        print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b375df-254a-4f6f-9889-6d52b885d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = \"Input.xlsx\"  # Replace with the path to your Excel file\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Sort the DataFrame by URL column\n",
    "df_sorted = df.sort_values(by='URL', ascending=True)\n",
    "\n",
    "# Create a folder to save the text files\n",
    "output_folder = \"extracted_texts\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Iterate over the sorted URL column\n",
    "for index, row in df_sorted.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}\n",
    "    webpage = requests.get(url, headers=headers).text\n",
    "\n",
    "    soup = BeautifulSoup(webpage, 'lxml')\n",
    "    soup.find_all('h1')[0].text.strip('\\n''\\t')\n",
    "\n",
    "    div_tag = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "\n",
    "    # Check if the <div> tag exists\n",
    "    if div_tag:\n",
    "        # Remove the text within <pre> tags\n",
    "        for pre_tag in div_tag.find_all('pre'):\n",
    "            pre_tag.extract()\n",
    "\n",
    "        # Extract all the text from the <div> tag\n",
    "        text = div_tag.get_text(separator='\\n', strip=True)\n",
    "\n",
    "        # Save the extracted text to a text file\n",
    "        file_path = os.path.join(output_folder, f\"{url_id}.txt\")\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6d7135-c522-456d-b613-494c6c9e5e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted from blackassign0045 and saved to extracted_texts/blackassign0045.txt\n",
      "Text extracted from blackassign0041 and saved to extracted_texts/blackassign0041.txt\n",
      "Text extracted from blackassign0044 and saved to extracted_texts/blackassign0044.txt\n",
      "Text extracted from blackassign0046 and saved to extracted_texts/blackassign0046.txt\n",
      "Text extracted from blackassign0028 and saved to extracted_texts/blackassign0028.txt\n",
      "Text extracted from blackassign0062 and saved to extracted_texts/blackassign0062.txt\n",
      "Text extracted from blackassign0059 and saved to extracted_texts/blackassign0059.txt\n",
      "Text extracted from blackassign0069 and saved to extracted_texts/blackassign0069.txt\n",
      "Text extracted from blackassign0081 and saved to extracted_texts/blackassign0081.txt\n",
      "Text extracted from blackassign0098 and saved to extracted_texts/blackassign0098.txt\n",
      "Text extracted from blackassign0088 and saved to extracted_texts/blackassign0088.txt\n",
      "Text extracted from blackassign0089 and saved to extracted_texts/blackassign0089.txt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m webpage \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     26\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(webpage, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m div_tag \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd-post-content tagdiv-type\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Check if the <div> tag exists\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = \"Input.xlsx\"  # Replace with the path to your Excel file\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Sort the DataFrame by URL column\n",
    "df_sorted = df.sort_values(by='URL', ascending=True)\n",
    "\n",
    "# Create a folder to save the text files\n",
    "output_folder = \"extracted_texts\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Iterate over the sorted URL column\n",
    "for index, row in df_sorted.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}\n",
    "    webpage = requests.get(url, headers=headers).text\n",
    "\n",
    "    soup = BeautifulSoup(webpage, 'lxml')\n",
    "    soup.find_all('h1')[0].text.strip('\\n''\\t')\n",
    "\n",
    "    div_tag = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "\n",
    "    # Check if the <div> tag exists\n",
    "    if div_tag:\n",
    "        # Remove the text within <pre> tags\n",
    "        for pre_tag in div_tag.find_all('pre'):\n",
    "            pre_tag.extract()\n",
    "\n",
    "        # Extract all the text from the <div> tag\n",
    "        text = div_tag.get_text(separator='\\n', strip=True)\n",
    "\n",
    "        # Save the extracted text to a text file\n",
    "        file_path = os.path.join(output_folder, f\"{url_id}.txt\")\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        \n",
    "        # Print message indicating the file name\n",
    "        print(f\"Text extracted from {url_id} and saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ec16b-c668-4f39-8623-37f4e3222261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = \"Input.xlsx\"  # Replace with the path to your Excel file\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Sort the DataFrame by URL column\n",
    "df_sorted = df.sort_values(by='URL', ascending=True)\n",
    "\n",
    "# Create a folder to save the text files\n",
    "output_folder = \"extracted_texts\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Iterate over the sorted URL column\n",
    "for index, row in df_sorted.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}\n",
    "    \n",
    "    try:\n",
    "        # Request the webpage\n",
    "        webpage = requests.get(url, headers=headers).text\n",
    "\n",
    "        # Parse the webpage\n",
    "        soup = BeautifulSoup(webpage, 'lxml')\n",
    "        soup.find_all('h1')[0].text.strip('\\n''\\t')\n",
    "\n",
    "        # Find the div tag\n",
    "        div_tag = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "\n",
    "        # Check if the <div> tag exists\n",
    "        if div_tag:\n",
    "            # Remove the text within <pre> tags\n",
    "            for pre_tag in div_tag.find_all('pre'):\n",
    "                pre_tag.extract()\n",
    "\n",
    "            # Extract all the text from the <div> tag\n",
    "            text = div_tag.get_text(separator='\\n', strip=True)\n",
    "\n",
    "            # Save the extracted text to a text file\n",
    "            file_path = os.path.join(output_folder, f\"{url_id}.txt\")\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "            \n",
    "            # Print message indicating the file name\n",
    "            print(f\"Text extracted from {url_id} and saved to {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing URL {url_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b92562d-8a45-4438-a117-c464792075be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted from blackassign0045 and saved to extracted_texts/blackassign0045.txt\n",
      "Text extracted from blackassign0041 and saved to extracted_texts/blackassign0041.txt\n",
      "Text extracted from blackassign0044 and saved to extracted_texts/blackassign0044.txt\n",
      "Text extracted from blackassign0046 and saved to extracted_texts/blackassign0046.txt\n",
      "Text extracted from blackassign0028 and saved to extracted_texts/blackassign0028.txt\n",
      "Text extracted from blackassign0062 and saved to extracted_texts/blackassign0062.txt\n",
      "Text extracted from blackassign0059 and saved to extracted_texts/blackassign0059.txt\n",
      "Text extracted from blackassign0069 and saved to extracted_texts/blackassign0069.txt\n",
      "Text extracted from blackassign0081 and saved to extracted_texts/blackassign0081.txt\n",
      "Text extracted from blackassign0098 and saved to extracted_texts/blackassign0098.txt\n",
      "Text extracted from blackassign0088 and saved to extracted_texts/blackassign0088.txt\n",
      "Text extracted from blackassign0089 and saved to extracted_texts/blackassign0089.txt\n",
      "Error occurred while processing URL blackassign0049: list index out of range\n",
      "Text extracted from blackassign0038 and saved to extracted_texts/blackassign0038.txt\n",
      "Text extracted from blackassign0052 and saved to extracted_texts/blackassign0052.txt\n",
      "Text extracted from blackassign0060 and saved to extracted_texts/blackassign0060.txt\n",
      "Text extracted from blackassign0050 and saved to extracted_texts/blackassign0050.txt\n",
      "Text extracted from blackassign0091 and saved to extracted_texts/blackassign0091.txt\n",
      "Text extracted from blackassign0047 and saved to extracted_texts/blackassign0047.txt\n",
      "Text extracted from blackassign0094 and saved to extracted_texts/blackassign0094.txt\n",
      "Text extracted from blackassign0074 and saved to extracted_texts/blackassign0074.txt\n",
      "Text extracted from blackassign0073 and saved to extracted_texts/blackassign0073.txt\n",
      "Text extracted from blackassign0022 and saved to extracted_texts/blackassign0022.txt\n",
      "Text extracted from blackassign0042 and saved to extracted_texts/blackassign0042.txt\n",
      "Text extracted from blackassign0051 and saved to extracted_texts/blackassign0051.txt\n",
      "Text extracted from blackassign0048 and saved to extracted_texts/blackassign0048.txt\n",
      "Text extracted from blackassign0070 and saved to extracted_texts/blackassign0070.txt\n",
      "Text extracted from blackassign0019 and saved to extracted_texts/blackassign0019.txt\n",
      "Text extracted from blackassign0054 and saved to extracted_texts/blackassign0054.txt\n",
      "Text extracted from blackassign0037 and saved to extracted_texts/blackassign0037.txt\n",
      "Text extracted from blackassign0040 and saved to extracted_texts/blackassign0040.txt\n",
      "Error occurred while processing URL blackassign0036: list index out of range\n",
      "Text extracted from blackassign0053 and saved to extracted_texts/blackassign0053.txt\n",
      "Text extracted from blackassign0076 and saved to extracted_texts/blackassign0076.txt\n",
      "Text extracted from blackassign0085 and saved to extracted_texts/blackassign0085.txt\n",
      "Text extracted from blackassign0086 and saved to extracted_texts/blackassign0086.txt\n",
      "Text extracted from blackassign0071 and saved to extracted_texts/blackassign0071.txt\n",
      "Text extracted from blackassign0075 and saved to extracted_texts/blackassign0075.txt\n",
      "Text extracted from blackassign0039 and saved to extracted_texts/blackassign0039.txt\n",
      "Text extracted from blackassign0058 and saved to extracted_texts/blackassign0058.txt\n",
      "Text extracted from blackassign0068 and saved to extracted_texts/blackassign0068.txt\n",
      "Text extracted from blackassign0056 and saved to extracted_texts/blackassign0056.txt\n",
      "Text extracted from blackassign0097 and saved to extracted_texts/blackassign0097.txt\n",
      "Text extracted from blackassign0080 and saved to extracted_texts/blackassign0080.txt\n",
      "Text extracted from blackassign0079 and saved to extracted_texts/blackassign0079.txt\n",
      "Text extracted from blackassign0087 and saved to extracted_texts/blackassign0087.txt\n",
      "Text extracted from blackassign0077 and saved to extracted_texts/blackassign0077.txt\n",
      "Text extracted from blackassign0078 and saved to extracted_texts/blackassign0078.txt\n",
      "Text extracted from blackassign0035 and saved to extracted_texts/blackassign0035.txt\n",
      "Text extracted from blackassign0003 and saved to extracted_texts/blackassign0003.txt\n",
      "Text extracted from blackassign0072 and saved to extracted_texts/blackassign0072.txt\n",
      "Text extracted from blackassign0090 and saved to extracted_texts/blackassign0090.txt\n",
      "Text extracted from blackassign0034 and saved to extracted_texts/blackassign0034.txt\n",
      "Text extracted from blackassign0061 and saved to extracted_texts/blackassign0061.txt\n",
      "Text extracted from blackassign0021 and saved to extracted_texts/blackassign0021.txt\n",
      "Text extracted from blackassign0027 and saved to extracted_texts/blackassign0027.txt\n",
      "Text extracted from blackassign0082 and saved to extracted_texts/blackassign0082.txt\n",
      "Text extracted from blackassign0005 and saved to extracted_texts/blackassign0005.txt\n",
      "Text extracted from blackassign0017 and saved to extracted_texts/blackassign0017.txt\n",
      "Text extracted from blackassign0007 and saved to extracted_texts/blackassign0007.txt\n",
      "Text extracted from blackassign0009 and saved to extracted_texts/blackassign0009.txt\n",
      "Text extracted from blackassign0010 and saved to extracted_texts/blackassign0010.txt\n",
      "Text extracted from blackassign0004 and saved to extracted_texts/blackassign0004.txt\n",
      "Text extracted from blackassign0018 and saved to extracted_texts/blackassign0018.txt\n",
      "Text extracted from blackassign0013 and saved to extracted_texts/blackassign0013.txt\n",
      "Text extracted from blackassign0026 and saved to extracted_texts/blackassign0026.txt\n",
      "Text extracted from blackassign0025 and saved to extracted_texts/blackassign0025.txt\n",
      "Text extracted from blackassign0008 and saved to extracted_texts/blackassign0008.txt\n",
      "Text extracted from blackassign0011 and saved to extracted_texts/blackassign0011.txt\n",
      "Text extracted from blackassign0024 and saved to extracted_texts/blackassign0024.txt\n",
      "Text extracted from blackassign0016 and saved to extracted_texts/blackassign0016.txt\n",
      "Text extracted from blackassign0015 and saved to extracted_texts/blackassign0015.txt\n",
      "Text extracted from blackassign0012 and saved to extracted_texts/blackassign0012.txt\n",
      "Text extracted from blackassign0001 and saved to extracted_texts/blackassign0001.txt\n",
      "Text extracted from blackassign0002 and saved to extracted_texts/blackassign0002.txt\n",
      "Text extracted from blackassign0023 and saved to extracted_texts/blackassign0023.txt\n",
      "Text extracted from blackassign0057 and saved to extracted_texts/blackassign0057.txt\n",
      "Text extracted from blackassign0006 and saved to extracted_texts/blackassign0006.txt\n",
      "Text extracted from blackassign0093 and saved to extracted_texts/blackassign0093.txt\n",
      "Text extracted from blackassign0030 and saved to extracted_texts/blackassign0030.txt\n",
      "Text extracted from blackassign0066 and saved to extracted_texts/blackassign0066.txt\n",
      "Text extracted from blackassign0055 and saved to extracted_texts/blackassign0055.txt\n",
      "Text extracted from blackassign0096 and saved to extracted_texts/blackassign0096.txt\n",
      "Text extracted from blackassign0095 and saved to extracted_texts/blackassign0095.txt\n",
      "Text extracted from blackassign0031 and saved to extracted_texts/blackassign0031.txt\n",
      "Text extracted from blackassign0067 and saved to extracted_texts/blackassign0067.txt\n",
      "Text extracted from blackassign0033 and saved to extracted_texts/blackassign0033.txt\n",
      "Text extracted from blackassign0032 and saved to extracted_texts/blackassign0032.txt\n",
      "Text extracted from blackassign0063 and saved to extracted_texts/blackassign0063.txt\n",
      "Text extracted from blackassign0065 and saved to extracted_texts/blackassign0065.txt\n",
      "Text extracted from blackassign0064 and saved to extracted_texts/blackassign0064.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = \"Input.xlsx\"  # Replace with the path to your Excel file\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Sort the DataFrame by URL column\n",
    "df_sorted = df.sort_values(by='URL', ascending=True)\n",
    "\n",
    "# Create a folder to save the text files\n",
    "output_folder = \"extracted_texts\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Iterate over the sorted URL column\n",
    "for index, row in df_sorted.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}\n",
    "    \n",
    "    try:\n",
    "        # Request the webpage\n",
    "        webpage = requests.get(url, headers=headers).text\n",
    "\n",
    "        # Parse the webpage\n",
    "        soup = BeautifulSoup(webpage, 'lxml')\n",
    "\n",
    "        # Extract text from the <h1> tag\n",
    "        h1_text = soup.find_all('h1')[0].text.strip('\\n''\\t')\n",
    "\n",
    "        # Find the div tag\n",
    "        div_tag = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "\n",
    "        # Check if the <div> tag exists\n",
    "        if div_tag:\n",
    "            # Remove the text within <pre> tags\n",
    "            for pre_tag in div_tag.find_all('pre'):\n",
    "                pre_tag.extract()\n",
    "\n",
    "            # Extract all the text from the <div> tag\n",
    "            div_text = div_tag.get_text(separator='\\n', strip=True)\n",
    "\n",
    "            # Combine the text from <h1> and <div> tags\n",
    "            text = f\"Website title: {h1_text}\\n\\nArticle Text: \\n\\n{div_text}\"\n",
    "\n",
    "            # Save the extracted text to a text file\n",
    "            file_path = os.path.join(output_folder, f\"{url_id}.txt\")\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "            \n",
    "            # Print message indicating the file name\n",
    "            print(f\"Text extracted from {url_id} and saved to {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing URL {url_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "415f29f3-4257-4887-bfdd-ed3c139ead90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''jkhsdkjfhasdhfkl'''\n",
    "# print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed566b-a4ec-4ddd-afdb-6b18374a70ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb0a8d-0e7f-46f5-9e29-d417e0e726f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
